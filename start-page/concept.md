# ডিপ নিউরাল নেটওয়ার্কের লেয়ারিং কনসেপ্ট

যদিও আমরা এখানে একটা সিঙ্গেল লেয়ার নিয়ে কাজ করছি, তবে ডিপ লার্নিং নেটওয়ার্কে ইনপুট এবং আউটপুট লেয়ারের মধ্যে অনেকগুলো হিডেন লেয়ার থাকতে পারে। যেটাকে আমরা নেটওয়ার্কের ডেপথ বলতে পারি। যত বেশি নোডের লেয়ার এর ভেতর দিয়ে ডাটা পার হবে, ততগুলো প্যাটার্ন রিকগনিশন এর প্রসেস চলতে থাকবে এই হিডেন লেয়ার গুলোতে।

শুরুর দিকের নিউরাল নেটওয়ার্ক গুলো র গভীরতা বেশি ছিল না কারণ এইযে পার্সপ্ট্রন, যেটার মধ্যে একটা ইনপুট আরেকটা আউটপুট লেয়ার, খুব বেশি হলে একটা হিডেন লেয়ার থাকতে পারে এর মধ্যে। আমাদের একটা ধারণা আছে তিনটা লেয়ারের বেশি একটা নেটওয়ার্ক, \(যার মধ্যে ইনপুট এবং আউটপুট লেয়ার সহ\) তাকে আমরা বলতে পারি একটা ডিপ লার্নিং নেটওয়ার্ক। মোদ্দাকথা একটার বেশি হিডেন লেয়ার থাকলেই সেটা ডিপ লার্নিং নেটওয়ার্ক।

একটা ডিপ লার্নিং নেটওয়ার্কে প্রতিটা নোডের একেকটা লেয়ারকে ট্রেইন করে কিছু স্পেসিফিক সেটের ফিচার দিয়ে, যা আসলে আসে আগের লেয়ারের আউটপুট থেকে। ব্যাপারটা এরকম, প্রতিটা লেয়ার একেকটা ফিচার আলাদা করে হ্যান্ডেল করছে। নিউরাল নেটওয়ার্কের একেকটা লেয়ার থেকে যত বেশি সামনের লেয়ারে এগোবো - ততই সে কমপ্লেক্স ফিচারগুলোকে ঠিকমতো আইডেন্টিফাই করতে পারবে। এটা সে করতে পারে যেহেতু সে আগের লেয়ারে নিচের ফিচারগুলোকে একসাথে 'এগ্রিগেট' এবং 'কম্বাইন' করে যোগ করতে পারছে ফিচারগুলোকে। একটা উদাহরণ দেই বরং। 

ধরা যাক একটা ছবি থেকে আপনাকে 'আইডেন্টিফাই' মানে 'ট্যাগ' করতে হবে। আমাদের সেই ছবির প্রথম ইনপুট লেয়ারে পিক্সেল হিসেবে সবকিছুই যাবে। পরের লেয়ারে সে পিক্সেলের ইনটেনসিটি বুঝে সেগুলোকে বিভিন্ন ভাগে ফেলবে। এরপরের লেয়ারে কোন অংশগুলো ছবিতে 'ডিস্টিঙ্কট' মানে একদম অন্যরকম, সেগুলোকে আলাদা করে ফেলবে। এরপরের লেয়ারে ধরুন - চোখের একটা 'কোনা'কে আলাদা করে ফেলবে সে। অথবা নাকের একটা অংশ। এই অংশগুলোকে যখন জোড়া লাগাবে তখন পরের কয়েকটা লেয়ারে একটা পুরো চোখ অথবা নাক সে বুঝতে পারবে। চোখ, নাক, কপাল, চিবুক এইসব ঠিকমত বুঝতে পারলে পরের কয়েকটা লেয়ারে পুরো মুখমণ্ডল সে বুঝতে পারার কথা। 

![&#x99A;&#x9BF;&#x9A4;&#x9CD;&#x9B0;&#x983; &#x995;&#x9BF;&#x9AD;&#x9BE;&#x9AC;&#x9C7; &#x9AC;&#x9BF;&#x9AD;&#x9BF;&#x9A8;&#x9CD;&#x9A8; &#x9B2;&#x9C7;&#x9DF;&#x9BE;&#x9B0;&#x9C7; &#x986;&#x9B2;&#x9BE;&#x9A6;&#x9BE; &#x986;&#x9B2;&#x9BE;&#x9A6;&#x9BE; &#x9AB;&#x9BF;&#x99A;&#x9BE;&#x9B0; &#x98F;&#x995;&#x9CD;&#x9B8;&#x99F;&#x9CD;&#x9B0;&#x9CD;&#x9AF;&#x9BE;&#x995;&#x9CD;&#x99F; &#x9B9;&#x99A;&#x9CD;&#x99B;&#x9C7;](../.gitbook/assets/54.jpg)

বুজতেই পারছেন, শুরুর দিকের লেয়ারগুলোর কাজ সোজা, আস্তে আস্তে যত সামনের দিকে এগুবে - বিশেষ করে আউটপুট লেয়ার পর্যন্ত, ততোই ফিচারগুলো কমপ্লেক্স হতে থাকবে। সে কারণে প্রথম দিকের লেয়ার থেকে পরের দিকের লেয়ারগুলোর যে কোন অবজেক্টকে আইডেন্টিফাই করার ক্ষমতা বেশি। এদিকে যেহেতু আগের লেয়ারগুলো তার ছোট ছোট আইটেম কে জোড়া লাগিয়ে দিয়েছে, সে কারণে পরের লেয়ার গুলোর কাজ বেশ সহজ হয়ে যায়। 

এটাকে আমরা বলি ফিচার হায়ারার্কি। একটা অফিসের নিচের কর্মকর্তার কাজের যত চাপ, তার থেকে উপরের দিকের কর্মকর্তার কাজের চাপ বাড়তেই থাকে। এই হায়ারার্কি ফিচারগুলোর কম্প্লেক্সিটি এবং কাজের অ্যাবসট্রাকশন ভালো বুঝতে পারে উপরের দিকে লেয়ারগুলো। এ কারণেই ডিপ লার্নিং নেটওয়ার্কগুলো বিশাল বড় বড় অনেক ডাইমেনশনের ডাটাসেট নিয়ে হ্যান্ডেল করতে পারে। কোন কোন ডাটাসেটগুলোর মধ্যে বিলিয়ন প্যারামিটার থাকে - যাকে আসলে পার হতে হয় অনেক নন-লিনিয়ার ফাংশন। আমি সামনে নন-লিনিয়ার ফাংশন নিয়ে আলাপ করব।

আমাকে যেহেতু অনেক ডেটা নিয়ে কাজ করতে হয়, সেজন্য বলতে পারি পৃথিবীর অধিকাংশ ডাটাই হচ্ছে 'লেবেল ছাড়া' ডাটা, যাকে আমরা আন-স্ট্রাকচার্ড ডেটা বলি। ডিপ লার্নিং এর এই বিভিন্ন লেয়ারের বিভিন্ন ফিচার 'এক্সট্রাকশন' এর কারণে সে যেকোন ডাটার মধ্যে বিভিন্ন প্যাটার্ন স্ট্রাকচার খুঁজে পায়। আমাদের এই 'আন-স্ট্রাকচার্ড' ডাটার মধ্যে ছবি, ভিডিও, অডিও রেকর্ডিং, ইন্টারনেটের ওয়েবসাইটগুলোর কোটি কোটি টেক্সট, এই সব কিছুর মধ্যে কোন কোনটা সাথে সামঞ্জস্য আর কোন কোনটার মধ্যে ব্যত্যয় আছে সেটা বের করতে পারে বড় সমস্যা ছাড়াই। মানুষের মাথা নিজেই একটা বড় নিউরাল নেটওয়ার্ক, তবুও এই বিলিয়ন বিলিয়ন ডেটার মধ্যে এই ধরনের কাজ করা মানুষের জন্য দুষ্কর।

![&#x99A;&#x9BF;&#x9A4;&#x9CD;&#x9B0;&#x983; &#x995;&#x9BF;&#x9AD;&#x9BE;&#x9AC;&#x9C7; &#x9AA;&#x9BF;&#x995;&#x9CD;&#x9B8;&#x9C7;&#x9B2; &#x9A5;&#x9C7;&#x995;&#x9C7; &#x98F;&#x995;&#x99F;&#x9BE; &#x9AC;&#x9C7;&#x9DC;&#x9BE;&#x9B2;&#x995;&#x9C7; &#x99A;&#x9BF;&#x9A8;&#x9A4;&#x9C7; &#x9AA;&#x9BE;&#x9B0;&#x99B;&#x9C7; &#x9A1;&#x9BF;&#x9AA; &#x9B2;&#x9BE;&#x9B0;&#x9CD;&#x9A8;&#x9BF;&#x982; ](../.gitbook/assets/38.png)

সেই কারণেই একটা ডিপ লার্নিং মডেলকে আপনি যদি কোটি কোটি ছবি ইনপুট হিসেবে পাঠান, তাহলে দেখা যাবে - সে গাড়ির ছবিগুলোকে একদিকে, বিড়ালের ছবি আরেকদিকে, আপনার ছবি একদিকে, রাস্তা, নদীর ছবি একদিকে ফেলে দিয়ে ক্যাটেগরি করে ভাগ করে দেবে। আজকে আমাদের মোবাইল ফোনের যতো স্মার্ট ফটো এ্যালবাম দেখছি, তার পিছনে কাজ করছে এই নিউরাল নেটওয়ার্ক।

![&#x99A;&#x9BF;&#x9A4;&#x9CD;&#x9B0;&#x983; &#x9AC;&#x9C7;&#x9DC;&#x9BE;&#x9B2;&#x9C7;&#x9B0; &#x99B;&#x9AC;&#x9BF; &#x9A5;&#x9C7;&#x995;&#x9C7; &#x995;&#x9BF;&#x9AD;&#x9BE;&#x9AC;&#x9C7; &#x9AB;&#x9BF;&#x99A;&#x9BE;&#x9B0; &#x98F;&#x995;&#x9CD;&#x9B8;&#x99F;&#x9CD;&#x9B0;&#x9CD;&#x9AF;&#x9BE;&#x995;&#x9CD;&#x99F; &#x995;&#x9B0;&#x99B;&#x9C7; &#x9AC;&#x9BF;&#x9AD;&#x9BF;&#x9A8;&#x9CD;&#x9A8; &#x9B2;&#x9C7;&#x9DF;&#x9BE;&#x9B0;&#x9C7; ](../.gitbook/assets/76.png)

সেভাবে আপনি যদি সোশ্যাল মিডিয়াতে একটা প্রোডাক্ট নিয়ে বাংলা অথবা ইংরেজিতে কমেন্টের কথা বলেন, তাহলে কারা কারা ওই প্রডাক্ট কে পছন্দ মানে 'এনডোর্স' করছে আর কত শতাংশ এর ব্যাপারে রাগারাগি করছে সেটাও বের করা সমস্যা নয়। আমি সেন্টিমেন্ট অ্যানালাইসিস এর কথা বলছি। আপনার কাছে কতগুলো ইমেইল 'পজেটিভ' আর 'নেগেটিভ' সেটা বের করতে পারবে এই ডিপ লার্নিং। একটা কল সেন্টারে কত শতাংশ মানুষ রাগান্বিত অবস্থায় কথা বলে, তার পাশাপাশি কত শতাংশ লোক সার্ভিস পেয়ে সন্তুষ্ট বের করা যাবে ভয়েস রেকর্ড থেকে।

আরেকটা কথা বলি। আমরা যে মোবাইল ফোন ব্যবহার করি, সেখান থেকে যদি 'টাইম সিরিজ' ডাটা আমরা ডিপ লার্নিং এ অ্যানালাইসিস করি তাহলে কিন্তু প্রতিটা মানুষের শারীরিক, মানসিক স্বাস্থ্য, তার স্বভাব, সামনে কে কি করতে পারে, সেটার বেশ ভালো ধারণা পাওয়া যায়। একটা গাড়ি থেকে যদি আমরা কনস্ট্যান্ট ডাটা ফীড নেই, তাহলে গাড়ির কোন কোন পার্টস সামনে কবে থেকে কাজ করবেনা বা নষ্ট হয়ে যাবে সেটা প্রেডিক্ট করা যাবে আগে থেকে। একটা ডাটা সেন্টারের সামনে কোন কোন হার্ড ড্রাইভ ফেইল করবে, সেটা আগে থেকে প্রেডিক্ট করতে পারলে ডাটা লস হবার সম্ভাবনা কম। এগুলোই একেকটা বিজনেস অপুর্চুনিটি। 

আবারো বলছি, ডিপ লার্নিং নেটওয়ার্ক অসাধারণ কাজ করে 'অটোমেটিক ফিচার এক্সট্রেকশন' এ। মানুষের কোন সাহায্য ছাড়াই। এই কাজগুলো আমরা করেছি সাধারণ মেশিন লার্নিং এ। অনেক সময় ডাটা সাইন্টিস্টদের এই বড় কাজগুলো করতে বছর লাগতো, বিশেষ করে সেই ডোমেইনের এক্সপার্টিজ \(ডাক্তার, প্রকৌশলী, রেডিওগ্রাফার, ল্যাব অ্যানালিস্ট\) নেবার ক্ষেত্রে, সেখানে সেই 'ডোমেইন নলেজ' ছাড়াই এই জিনিসটা করা যাচ্ছে সহজে - ডিপ লার্নিং দিয়ে। এর সহজ ইন্টারপ্রিটেশন হচ্ছে, আপনি বিশাল একটা কাজের জন্য ছোট একটা ডাটা সাইন্স টিমকে 'অগমেন্ট' করতে পারবেন এই ডিপ লার্নিং এর সাহায্য নিয়ে।

![&#x99A;&#x9BF;&#x9A4;&#x9CD;&#x9B0;: &#x9B2;&#x9B8; &#x98F;&#x9AC;&#x982; &#x985;&#x9CD;&#x9AF;&#x9BE;&#x995;&#x9CD;&#x9AF;&#x9C1;&#x9B0;&#x9C7;&#x9B8;&#x9BF; &#x985;&#x9A8;&#x9C7;&#x995;&#x99F;&#x9BE;&#x987; &#x9B8;&#x9AE;&#x9BE;&#x9B0;&#x9CD;&#x9A5;&#x995; ](../.gitbook/assets/62.png)

যখন আমরা 'লেবেল ছাড়া' ডাটাকে ট্রেনিং করাই তখন ডিপ নেটওয়ার্কের প্রতিটা নোডের একেকটা লেয়ার ফিচারগুলোকে শিখতে থাকে স্বয়ংক্রিয়ভাবে - কারণ সে আসলে ইনপুট ডেটাকে আবারো 'রিকনস্ট্রাক্ট' বা নতুনভাবে তৈরি করতে চায় যাতে তার শুরুর নেটওয়ার্ক যেই 'ধারণা' মানে 'অ্যাসাম্পশন' করে তার পাশাপাশি তার প্রবাবিলিটি ডিস্ট্রিবিউশন এর সাথে মেলানোর চেষ্টা করে। এই কাজের মধ্যে নিউরাল নেটওয়ার্কগুলো দরকারি ফিচারগুলোর মধ্যে কোরিলেশন খুঁজে বের করে। এর ফলে একটা ভালো আউটপুট \(অ্যাক্যুরেসি\) এর জন্য যে ফিচার আউটপুট আর যে ফিচারগুলো থেকে শিখছে, এদের মধ্যে একটা ভালো সংযোগ খোঁজার চেষ্টা করে। এর ফলে সেটা মডেলের একটা নতুন 'রিকনস্ট্রাকশন' অথবা লেবেলসহ ডেটা আছে তার মধ্যে দূরত্ব কমিয়ে নিয়ে আসে।

একবার যদি ডিপ লার্নিং নেটওয়ার্ক ট্রেনিং করানো হয় লেবেল সহ ডাটার ওপর, তাহলে সেটাকে ব্যবহার করা যাবে নতুন একটা আন-স্ট্রাকচারড ডাটাসেটের উপর। আমাদের যদি মডেলের ভালো পারফরম্যান্স দরকার হয় তাহলে যত বেশি ডাটার ওপর নিউরাল নেটওয়ার্ককে ট্রেইন করানো হবে, তত বেশি মডেলের দক্ষতা বা অ্যাকুরেসি বাড়বে। এমনও গেছে - বাজে অ্যালগরিদম দিয়ে অনেক ডাটার ওপর ট্রেইন করালে সেটা আরেকটা ভালো অ্যালগরিদম যা কম ডাটার ওপর ট্রেইন করা হয়েছে তার থেকে ভালো কাজ করবে। ডিপ লার্নিং এর ক্ষমতা নির্ভর করে - যত বেশি লেবেল ছাড়া ডাটা দেয়া যাবে সেখানে সে অন্য যেকোনো অ্যালগরিদম থেকে ভালো কাজ করবে। যতো গুড়, ততো মিষ্টি, কথাটা এখানে খাটে। 

সবশেষে, ডিপ লার্নিং নেটওয়ার্কগুলো কিন্তু তার শেষ আউটকাম দেয় আউটপুট লেয়ারে। এখানে আমরা একটা লজিস্টিক অথবা সফটম্যাক্স অ্যাক্টিভেশন ফাংশন ক্লাসিফায়ার যা আসলে অন্যান্য আউটপুটের ভেতরে একটা প্রোবাবিলিটি ভোটিংয়ের মাধ্যমে আউটকাম বের করে দেয়। এই আউটপুট লেয়ারে একটা অ্যাক্টিভেশন ফাংশন দিয়ে নন-লিনিয়ার ডেটা থেকে তার প্রেডিকশন তৈরি করে। এর অর্থ হচ্ছে পেছনের ডাটা থেকে শিখে সে একটা ভবিষ্যদ্বাণী করতে পারে। একটা ছবি দিলে সে বলতে পারবে ৯০% অ্যাক্যুরেসিতে যে এটা আপনি। 



